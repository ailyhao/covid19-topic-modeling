# Covid-19 Topic Modeling 

## Data Resource
Data used in this topic modeling project is returned collections of COVID-19 relevant Tweets that are generated by [Twitter Standard Search API](https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets). The 4 csv files saved in [raw_data](./raw_data) include the raw data generated by the Twitter API and there are 8 columns in each file, which are `tweet_id`, `user`, `text`, `retweets`, `favorites`, `created_at`, `location` and `hashtags`. For this project, only `text` will be used as it contains all the relevant Tweets. All Tweets include the key word 'COVID'. 

## Data Preprocessing
### Remove key word 'covid' in each tweet using Regex

```python
tweets = raw_data.text.values.tolist()
tweets = [re.sub(r'\b(\w*[Cc][Oo][Vv][Ii][Dd]\w*)\b', '', tweet) for tweet in tweets]
```
- Remove punctuations and convert sentences to lists of words

```python
def tweets_to_words(sentences):
  for sentence in sentences:
    yield(gensim.utils.simple_preprocess(str(sentence), deacc = True)) # deacc = True removes punctuations

tweets_words = list(tweets_to_words(tweets))
```

### Remove stop words

```python
# NLTK Stop words
from nltk.corpus import stopwords
stop_words = stopwords.words('english')

def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

tweets_words_nostops = remove_stopwords(tweets_words)
```

### Build bigrams and trigrams

```python
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]


def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]

tweets_words_bigrams = make_bigrams(tweets_words_nostops)
```


### Lemmatization 

```python
def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """https://spacy.io/api/annotation"""
    texts_out = []
    for tweet in texts:
        doc = nlp(" ".join(tweet)) 
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out

tweets_lemmatized = lemmatization(tweets_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])
```

### Create Dictionary

```python
id2word = corpora.Dictionary(tweets_lemmatized)
```
### Create Corpus

```python
# Create Corpus
texts = tweets_lemmatized

# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]
```

## Build LDA model
Topic modeling is used to extract the hidden topics from large amounts of text. Latent Dirichlet Allocation (LDA) is a popular algorithm for topic modeling in the Python's Gensim package. 

```python
# Build LDA model
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=3, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True)
```

Model perplexity and topic coherence prvide a convenient measure to judge how good a topic model is. The higher the coherence score the better the model performs. 

```python
# Compute Perplexity
print('\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=tweets_lemmatized, dictionary=id2word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)
```

## Visualization 

<img src= img/topic_vis.png />
